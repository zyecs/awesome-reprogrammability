# Awesome Neural Network Reprogrammability [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

> A curated list of resources on neural network reprogrammability - techniques for reprogramming neural networks to perform new tasks without full retraining.

Neural network reprogrammability refers to methods that enable pre-trained neural networks to be adapted or "reprogrammed" for new tasks with minimal parameter updates, often leveraging the existing learned representations. This includes techniques like input transformations, prompt-based approaches, adapter methods, and parameter-efficient fine-tuning strategies.

## Contents

- [Papers](#papers)
- [Datasets](#datasets)
- [Benchmarks](#benchmarks)
- [Tools & Libraries](#tools--libraries)
- [Leaderboards](#leaderboards)
- [Contributing](#contributing)

<!-- AUTO:START -->


## Curated Papers (auto-generated)


### Model Reprogramming

*19 papers*

- [Attribute-based Visual Reprogramming for Vision-Language Models (2025) — ICLR](#attribute-based-visual-reprogramming-for-vision-language-models)
- [Model Reprogramming Demystified: A Neural Tangent Kernel Perspective (2025) — arXiv preprint arXiv:2506.0620](https://arxiv.org/abs/2506.0620)
- [Refine: Inversion-free backdoor defense via model reprogramming (2025) — ICLR](#refine-inversion-free-backdoor-defense-via-model-reprogramming)
- [Reprogramming pretrained language models for protein sequence representation learning (2025) — Digital Discovery](#reprogramming-pretrained-language-models-for-protein-sequence-representation-learning)
- [Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts (2025) — ICML](#understanding-model-reprogramming-for-clip-via-decoupling-visual-prompts)
- [Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders (2024) — SaTML](#model-reprogramming-outperforms-fine-tuning-on-out-of-distribution-data-in-text-image-encoders)
- [Model reprogramming: Resource-efficient cross-domain machine learning (2024) — AAAI](#model-reprogramming-resource-efficient-cross-domain-machine-learning)
- [Time-llm: Time series forecasting by reprogramming large language models (2024) — ICLR](#time-llm-time-series-forecasting-by-reprogramming-large-language-models)
- [Deep graph reprogramming (2023) — CVPR](#deep-graph-reprogramming)
- [From english to more languages: Parameter-efficient model reprogramming for cross-lingual speech recognition (2023) — ICASSP](#from-english-to-more-languages-parameter-efficient-model-reprogramming-for-cross-lingual-speech-recognition)
- [Low-resource music genre classification with cross-modal neural model reprogramming (2023) — ICASSP](#low-resource-music-genre-classification-with-cross-modal-neural-model-reprogramming)
- [Reprogramming pretrained language models for antibody sequence infilling (2023) — ICML](#reprogramming-pretrained-language-models-for-antibody-sequence-infilling)
- [Adversarial Reprogramming Revisited (2022) — NeurIPS](#adversarial-reprogramming-revisited)
- [Cross-modal adversarial reprogramming (2022) — CVPR](#cross-modal-adversarial-reprogramming)
- [Voice2series: Reprogramming acoustic models for time series classification (2021) — ICML](#voice2series-reprogramming-acoustic-models-for-time-series-classification)
- [WARP: Word-level adversarial reprogramming (2021) — ACL-IJCNLP](#warp-word-level-adversarial-reprogramming)
- [Reprogramming Language Models for Molecular Representation Learning (2020) — NeurIPS](#reprogramming-language-models-for-molecular-representation-learning)
- [Transfer Learning without Knowing: Reprogramming Black-box Machine Learning Models with Scarce Data and Limited Resources (2020) — ICML](#transfer-learning-without-knowing-reprogramming-black-box-machine-learning-models-with-scarce-data-and-limited-resources)
- [Adversarial Reprogramming of Text Classification Neural Networks (2019) — EMNLP/IJCNLP](#adversarial-reprogramming-of-text-classification-neural-networks)

### Prompt Tuning

*11 papers*

- [Universal prompt tuning for graph neural networks (2023) — NeurIPS](#universal-prompt-tuning-for-graph-neural-networks)
- [Differentiable prompt makes pre-trained language models better few-shot learners (2022) — ICLR](#differentiable-prompt-makes-pre-trained-language-models-better-few-shot-learners)
- [Learning to Prompt for Vision-Language Models (2022) — IJCV](https://arxiv.org/abs/2109.01134) · [code](https://github.com/KaiyangZhou/CoOp)
- [P-Tuning: Prompt Tuning Can Be Comparable to Fine-Tuning Across Scales and Tasks (2022) — ACL, Volume 2: Short Papers](#p-tuning-prompt-tuning-can-be-comparable-to-fine-tuning-across-scales-and-tasks)
- [PPT: Pre-trained Prompt Tuning for Few-shot Learning (2022) — Proceedings of ACL](#ppt-pre-trained-prompt-tuning-for-few-shot-learning)
- [PTR: Prompt Tuning with Rules for Text Classification (2022) — AI Open](#ptr-prompt-tuning-with-rules-for-text-classification)
- [Learning how to ask: Querying LMs with mixtures of soft prompts (2021) — arXiv preprint arXiv:2104.06599](https://arxiv.org/abs/2104.06599)
- [P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks (2021) — arXiv preprint arXiv:2110.07602](https://arxiv.org/abs/2110.07602)
- [Prefix-Tuning: Optimizing Continuous Prompts for Generation (2021) — ACL/IJCNLP](#prefix-tuning-optimizing-continuous-prompts-for-generation)
- [Spot: Better frozen model adaptation through soft prompt transfer (2021) — arXiv preprint arXiv:2110.07904](https://arxiv.org/abs/2110.07904)
- [The Power of Scale for Parameter-Efficient Prompt Tuning (2021) — EMNLP](https://arxiv.org/abs/2104.08691) · [code](https://github.com/google-research/prompt-tuning)

### Prompt Instruction

*4 papers*

- [Visual instruction tuning (2023) — NeurIPS](#visual-instruction-tuning)
- [Finetuned Language Models are Zero-Shot Learners (2022) — ICLR](#finetuned-language-models-are-zero-shot-learners)
- [Multitask Prompted Training Enables Zero-Shot Task Generalization (2022) — ICLR](#multitask-prompted-training-enables-zero-shot-task-generalization)
- [Training language models to follow instructions with human feedback (2022) — NeurIPS](#training-language-models-to-follow-instructions-with-human-feedback)

### Unknown

*1 papers*

- [Adversarial Reprogramming of Neural Networks (2019) — ICLR](https://arxiv.org/abs/1806.11146)


_Generated: 2025-09-10 15:27 UTC_

<!-- AUTO:END -->

## Contributing

Contributions are welcome! Please read the [contribution guidelines](CONTRIBUTING.md) first.

## Seeding From BibTeX (optional)

Convert BibTeX to YAML and refresh the README:

```bash
python scripts/bibtex_to_yaml.py my_refs.bib --out lists/papers.yaml --append \
  --category "prompt tuning"
python scripts/validate_lists.py
python scripts/render_readme.py
```

Flags: `--overwrite` to replace `lists/papers.yaml`; `--category` to force the mechanism for all entries (`model reprogramming`, `prompt tuning`, `prompt instruction`); `--default-mechanism/--default-location/--default-operator` to set fallbacks. Review tags to match `meta/tags.md`.

## License

[![CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)

This work is licensed under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).
