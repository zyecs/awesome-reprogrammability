# Papers

A tabular view of curated papers organized by the reprogrammability taxonomy dimensions.

Paper | Configuration ($\lambda$) | Location ($\ell$) | Operator ($\tau$) | Alignment ($\omega$) | Venue
--- | --- | --- | --- | --- | ---
[Adversarial Reprogramming of Neural Networks](https://arxiv.org/abs/1806.11146) Elsayed et al. (2019) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Statistical (SA) | ICLR
[Adversarial Reprogramming of Text Classification Neural Networks](https://arxiv.org/abs/1809.01829) Neekhara et al. (2019) | Learnable | Embedding ($\mathcal{E}$) | Parametric (PR) | Statistical (SA) / Linear (LA) | EMNLP/IJCNLP
[Reprogramming Language Models for Molecular Representation Learning](https://arxiv.org/abs/2012.03460) Vinod et al. (2020) | Learnable | Input ($\mathcal{X}_S$) | Parametric (PR) | Rule-based (RA) | NeurIPS Workshop
[Learning how to ask: Querying LMs with mixtures of soft prompts](https://arxiv.org/abs/2104.06599) Qin et al. (2021) | Learnable | Embedding ($\mathcal{E}$) | Concatenative (CO) | Identity (ID) | NAACL
[P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks](https://arxiv.org/abs/2110.07602) Liu et al. (2021) | Learnable | Hidden ($\mathcal{H}$) | Concatenative (CO) | Linear (LA) | ACL
[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190) Li et al. (2021) | Learnable | Embedding ($\mathcal{E}$) | Concatenative (CO) | Identity (ID) | ACL/IJCNLP
[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691) Lester et al. (2021) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Identity (ID) | EMNLP
[Transfer Learning without Knowing: Reprogramming Black-box Machine Learning Models with Scarce Data and Limited Resources](https://arxiv.org/abs/2007.08714) Tsai et al. (2021) | Learnable | input-layers | statistical / linear | Identity (ID) | ICML
[Voice2series: Reprogramming acoustic models for time series classification](https://arxiv.org/abs/2106.09296) Yang et al. (2021) | Learnable | Input ($\mathcal{X}_S$) | Parametric (PR) | Statistical (SA) | ICML
[WARP: Word-level Adversarial ReProgramming](https://arxiv.org/abs/2101.00121) Hambardzumyan et al. (2021) | Learnable | Input ($\mathcal{X}_S$) | Concatenative (CO) | Linear (LA) | ACL / ACL-IJCNLP
[Adversarial Reprogramming Revisited](https://arxiv.org/abs/2206.03466) Englert et al. (2022) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Statistical (SA) | NeurIPS
[An Explanation of In-context Learning as Implicit Bayesian Inference](https://arxiv.org/abs/2111.02080) Xie et al. (2022) | Learnable | Input ($\mathcal{X}_S$) | Concatenative (CO) | Identity (ID) | ICLR
[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) Wei et al. (2022) | Fixed | input-space | Concatenative (CO) | Identity (ID) | NeurIPS
[Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557) Zhou et al. (2022) | Learnable | Embedding ($\mathcal{E}$) | Concatenative (CO) | Identity (ID) / Linear (LA) | CVPR
[Cross-modal Adversarial Reprogramming](https://arxiv.org/abs/2102.07325) Neekhara et al. (2022) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Linear (LA) | WACV
[Differentiable prompt makes pre-trained language models better few-shot learners](#differentiable-prompt-makes-pre-trained-language-models-better-few-shot-learners) Zhang et al. (2022) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Identity (ID) | ICLR
[Exploring Visual Prompts for Adapting Large-Scale Models](https://arxiv.org/abs/2203.17274) Bahng et al. (2022) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Statistical (SA) | arXiv
[Finetuned Language Models are Zero-Shot Learners](#finetuned-language-models-are-zero-shot-learners) Wei et al. (2022) | Fixed | Hidden ($\mathcal{H}$) | Additive (AD) | Identity (ID) | ICLR
[In-context Learning and Induction Heads](https://arxiv.org/abs/2209.11895) OLSSON et al. (2022) | Fixed | Input ($\mathcal{X}_S$) | Concatenative (CO) | Identity (ID) | arXiv
[Learning To Retrieve Prompts for In-Context Learning](https://arxiv.org/abs/2112.08633) Rubin et al. (2022) | Learnable | input-space | Concatenative (CO) | Identity (ID) | NAACL
[Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134) Zhou et al. (2022) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Identity (ID) | IJCV
[Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134) Zhou et al. (2022) | Learnable | Embedding ($\mathcal{E}$) | Concatenative (CO) | Linear (LA) | IJCV
[Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625) Zhou et al. (2022) | Fixed | Input ($\mathcal{X}_S$) | Concatenative (CO) | Identity (ID) | ICLR
[Multitask Prompted Training Enables Zero-Shot Task Generalization](#multitask-prompted-training-enables-zero-shot-task-generalization) Sanh et al. (2022) | Fixed | Hidden ($\mathcal{H}$) | Additive (AD) | Identity (ID) | ICLR
[P-Tuning: Prompt Tuning Can Be Comparable to Fine-Tuning Across Scales and Tasks](#p-tuning-prompt-tuning-can-be-comparable-to-fine-tuning-across-scales-and-tasks) Liu et al. (2022) | Learnable | Input ($\mathcal{X}_S$) | multiplication | Identity (ID) | ACL, Volume 2: Short Papers
[PPT: Pre-trained Prompt Tuning for Few-shot Learning](#ppt-pre-trained-prompt-tuning-for-few-shot-learning) Gu et al. (2022) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Identity (ID) | Proceedings of ACL
[PTR: Prompt Tuning with Rules for Text Classification](#ptr-prompt-tuning-with-rules-for-text-classification) Han et al. (2022) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Identity (ID) | AI Open
[Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837) MIN et al. (2022) | Fixed | Input ($\mathcal{X}_S$) | Concatenative (CO) | Rule-based (RA) | EMNLP
[Spot: Better frozen model adaptation through soft prompt transfer](https://arxiv.org/abs/2110.07904) Vu et al. (2022) | Learnable | Embedding ($\mathcal{E}$) / Hidden ($\mathcal{H}$) | Concatenative (CO) | Linear (LA) | ACL
[Structured Prompting: Scaling In-Context Learning to 1,000 Examples](https://arxiv.org/abs/2212.06713) HAO et al. (2022) | Fixed | Hidden ($\mathcal{H}$) | Concatenative (CO) | Identity (ID) | arXiv
[Training language models to follow instructions with human feedback](#training-language-models-to-follow-instructions-with-human-feedback) Ouyang et al. (2022) | Fixed | Hidden ($\mathcal{H}$) | Additive (AD) | Identity (ID) | NeurIPS
[Unleashing the Power of Visual Prompting At the Pixel Level](https://arxiv.org/abs/2212.10556) WU et al. (2022) | Learnable | Input ($\mathcal{X}_S$) | Concatenative (CO) | Identity (ID) / Statistical (SA) | arXiv
[Visual Prompt Tuning](https://arxiv.org/abs/2203.12119) JIA et al. (2022) | Fixed | Embedding ($\mathcal{E}$) / Hidden ($\mathcal{H}$) | Concatenative (CO) | Linear (LA) | ECCV
[Visual Prompting via Image Inpainting](https://arxiv.org/abs/2209.00647) BAR et al. (2022) | Fixed | Input ($\mathcal{X}_S$) | Concatenative (CO) | Identity (ID) | NeurIPS
[A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models](https://arxiv.org/abs/2302.06235) ALLINGHAM et al. (2023) | Fixed | Input ($\mathcal{X}_S$) | Concatenative (CO) | Identity (ID) | ICML
[BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning](https://arxiv.org/abs/2303.14773) OH et al. (2023) | Learnable | input-space | Additive (AD) | Rule-based (RA) | CVPR
[Decomposed Prompting: A Modular Approach for Solving Complex Tasks](https://arxiv.org/abs/2210.02406) Khot et al. (2023) | Fixed | Input ($\mathcal{X}_S$) | Concatenative (CO) | Identity (ID) | ICLR
[Deep graph reprogramming](#deep-graph-reprogramming) Jing et al. (2023) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Identity (ID) | CVPR
[Explicit Visual Prompting for Low-Level Structure Segmentations](https://arxiv.org/abs/2303.10883) Liu et al. (2023) | Learnable | Embedding ($\mathcal{E}$) / Hidden ($\mathcal{H}$) | Parametric (PR) | Identity (ID) | CVPR
[From english to more languages: Parameter-efficient model reprogramming for cross-lingual speech recognition](#from-english-to-more-languages-parameter-efficient-model-reprogramming-for-cross-lingual-speech-recognition) Yang et al. (2023) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Identity (ID) | ICASSP
[InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) Dai et al. (2023) | Learnable | Embedding ($\mathcal{E}$) | Parametric (PR) | Identity (ID) / rule / Linear (LA) | NeurIPS
[Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions](https://arxiv.org/abs/2212.10509) TRIVEDI et al. (2023) | Fixed | Input ($\mathcal{X}_S$) | Concatenative (CO) | Identity (ID) | ACL
[Low-resource music genre classification with cross-modal neural model reprogramming](#low-resource-music-genre-classification-with-cross-modal-neural-model-reprogramming) Hung et al. (2023) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Identity (ID) | ICASSP
[MaPLe: Multi-modal Prompt Learning](https://arxiv.org/abs/2210.03117) Khattak et al. (2023) | Learnable | Embedding ($\mathcal{E}$) / Hidden ($\mathcal{H}$) | Concatenative (CO) | Linear (LA) | CVPR
[Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Recognition](https://arxiv.org/abs/2110.03894) Yen et al. (2023) | Learnable | input-space | Additive (AD) | Statistical (SA) | Interspeech
[On the Role of Attention in Prompt-tuning](https://arxiv.org/abs/2306.03435) OYMAK et al. (2023) | Learnable | Embedding ($\mathcal{E}$) / Hidden ($\mathcal{H}$) | Concatenative (CO) | Linear (LA) | ICML 2023
[PLOT: Prompt Learning with Optimal Transport for Vision-Language Models](https://arxiv.org/abs/2210.01253) CHEN et al. (2023) | Learnable | Embedding ($\mathcal{E}$) | Concatenative (CO) | Identity (ID) | ICLR
[Reprogramming pretrained language models for antibody sequence infilling](#reprogramming-pretrained-language-models-for-antibody-sequence-infilling) Melnyk et al. (2023) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Identity (ID) | ICML
[Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V](https://arxiv.org/abs/2310.11441) YANG et al. (2023) | Fixed | Input ($\mathcal{X}_S$) | Additive (AD) | Rule-based (RA) | arXiv
[TransHP: Image Classification with Hierarchical Prompting](https://arxiv.org/abs/2304.06385) WANG et al. (2023) | Learnable | Embedding ($\mathcal{E}$) / Hidden ($\mathcal{H}$) | Concatenative (CO) | Linear (LA) | NeurIPS
[Tuning Multi-mode Token-level Prompt Alignment across Modalities](https://arxiv.org/abs/2309.13847) WANG et al. (2023) | Learnable | Embedding ($\mathcal{E}$) | Concatenative (CO) | Identity (ID) | NeurIPS 2023
[Understanding and Improving Visual Prompting: A Label-Mapping Perspective](https://arxiv.org/abs/2211.11635) CHEN et al. (2023) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Statistical (SA) | CVPR
[Universal prompt tuning for graph neural networks](#universal-prompt-tuning-for-graph-neural-networks) Fang et al. (2023) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Identity (ID) | NeurIPS
[Visual instruction tuning](#visual-instruction-tuning) Liu et al. (2023) | Fixed | Hidden ($\mathcal{H}$) | Additive (AD) | Identity (ID) | NeurIPS
[What Does a Platypus Look Like? Generating Customized Prompts for Zero-Shot Image Classification](https://arxiv.org/abs/2209.03320) PRATT et al. (2023) | Fixed | Input ($\mathcal{X}_S$) | Concatenative (CO) | Identity (ID) | ICCV
[What Makes Good Examples for Visual In-Context Learning?](https://arxiv.org/abs/2301.13670) ZHANG et al. (2023) | Fixed | Input ($\mathcal{X}_S$) | Concatenative (CO) | Identity (ID) | arXiv
[ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2311.16494) TIAN et al. (2024) | Learnable | Embedding ($\mathcal{E}$) | Concatenative (CO) | Identity (ID) | CVPR
[AutoVP: An Automated Visual Prompting Framework and Benchmark](https://arxiv.org/abs/2310.08381) TSAO et al. (2024) | Learnable | Input ($\mathcal{X}_S$) | Concatenative (CO) | Statistical (SA) / Linear (LA) | ICLR
[Bayesian-guided Label Mapping for Visual Reprogramming](https://arxiv.org/abs/2410.24018) CAI et al. (2024) | Learnable | input-space | Additive (AD) | Statistical (SA) | NeurIPS
[Exploring the Transferability of Visual Prompting for Multimodal Large Language Models](https://arxiv.org/abs/2404.11207) Zhang et al. (2024) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Statistical (SA) | CVPR
[Joint Visual and Text Prompting for Improved Object-Centric Perception with Multimodal Large Language Models](https://arxiv.org/abs/2404.04514) Jiang et al. (2024) | Fixed | Input ($\mathcal{X}_S$) / Embedding ($\mathcal{E}$) | addition / concatenation | Identity (ID) | arXiv
[Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders](https://arxiv.org/abs/2403.10800) GENG et al. (2024) | Learnable | Input ($\mathcal{X}_S$) / Embedding ($\mathcal{E}$) | addition / parametric | Identity (ID) | SatML 2024
[PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs](https://arxiv.org/abs/2402.07872) NASIRIANY et al. (2024) | Fixed | Input ($\mathcal{X}_S$) | Additive (AD) | Rule-based (RA) | ICML
[PromptKD: Unsupervised Prompt Distillation for Vision-Language Models](https://arxiv.org/abs/2403.02781) LI et al. (2024) | Learnable | Embedding ($\mathcal{E}$) | Concatenative (CO) | Identity (ID) | CVPR
[Sample-specific Masks for Visual Reprogramming-based Prompting](https://arxiv.org/abs/2406.03150) Cai et al. (2024) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Statistical (SA) | ICML
[Time-llm: Time series forecasting by reprogramming large language models](#time-llm-time-series-forecasting-by-reprogramming-large-language-models) Jin et al. (2024) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Identity (ID) | ICLR
[When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations](https://arxiv.org/pdf/2310.19698) PETROV et al. (2024) | Learnable | Input ($\mathcal{X}_S$) / Embedding ($\mathcal{E}$) / Hidden ($\mathcal{H}$) | Concatenative (CO) | Identity (ID) | ICLR
[Attribute-based Visual Reprogramming for Vision-Language Models](https://arxiv.org/abs/2501.13982) Cai et al. (2025) | Learnable | Input ($\mathcal{X}_S$) | addition / concatenation | Rule-based (RA) | ICLR
[Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want](https://arxiv.org/abs/2403.20271) Lin et al. (2025) | Learnable | embedding-level | Parametric (PR) | Linear (LA) | ICLR
[Model Reprogramming Demystified: A Neural Tangent Kernel Perspective](https://arxiv.org/abs/2506.0620) Chung et al. (2025) | Learnable | input-layers | Additive (AD) | Identity (ID) | arXiv
[Refine: Inversion-free backdoor defense via model reprogramming](https://arxiv.org/abs/2502.18508) Chen et al. (2025) | Learnable | input-layers | Additive (AD) | Identity (ID) | ICLR
[Reprogramming pretrained language models for protein sequence representation learning](https://arxiv.org/abs/2301.02120) Vinod et al. (2025) | Learnable | input-layers | Additive (AD) | Identity (ID) | Digital Discovery
[Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts](https://arxiv.org/abs/2506.01000) CAI et al. (2025) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Linear (LA) | ICML 2025
