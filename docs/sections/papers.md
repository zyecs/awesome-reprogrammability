# Papers

A tabular view of curated papers organized by the reprogrammability taxonomy dimensions.

Paper | Configuration ($\lambda$) | Location ($\ell$) | Operator ($\tau$) | Alignment ($\omega$) | Venue
--- | --- | --- | --- | --- | ---
[Adversarial Reprogramming of Neural Networks](https://arxiv.org/abs/1806.11146) Elsayed et al. (2019) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Statistical (SA) | ICLR
[Adversarial Reprogramming of Text Classification Neural Networks](https://arxiv.org/abs/1809.01829) Neekhara et al. (2019) | Learnable | Embedding ($\mathcal{E}$) | Parametric (PR) | Statistical (SA) / Linear (LA) | EMNLP/IJCNLP
[Reprogramming Language Models for Molecular Representation Learning](https://arxiv.org/abs/2012.03460) Vinod et al. (2020) | Learnable | Input ($\mathcal{X}_S$) | Parametric (PR) | Rule-based (RA) | NeurIPS Workshop
[Transfer Learning without Knowing: Reprogramming Black-box Machine Learning Models with Scarce Data and Limited Resources](https://arxiv.org/abs/2007.08714) Tsai et al. (2020) | Learnable | input-layers | statistical / linear | Linear (LA) | ICML
[Learning how to ask: Querying LMs with mixtures of soft prompts](https://arxiv.org/abs/2104.06599) Qin et al. (2021) | Learnable | Embedding ($\mathcal{E}$) | Concatenative (CO) | Identity (ID) | NAACL
[P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks](https://arxiv.org/abs/2110.07602) Liu et al. (2021) | Learnable | Hidden ($\mathcal{H}$) | Concatenative (CO) | Linear (LA) | ACL
[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190) Li et al. (2021) | Learnable | Embedding ($\mathcal{E}$) | Concatenative (CO) | Identity (ID) | ACL/IJCNLP
[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691) Lester et al. (2021) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Linear (LA) | EMNLP
[Voice2series: Reprogramming acoustic models for time series classification](#voice2series-reprogramming-acoustic-models-for-time-series-classification) Yang et al. (2021) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | ICML
[WARP: Word-level adversarial reprogramming](#warp-word-level-adversarial-reprogramming) Hambardzumyan et al. (2021) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | ACL-IJCNLP
[Adversarial Reprogramming Revisited](#adversarial-reprogramming-revisited) Englert et al. (2022) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | NeurIPS
[Cross-modal adversarial reprogramming](#cross-modal-adversarial-reprogramming) Neekhara et al. (2022) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | CVPR
[Differentiable prompt makes pre-trained language models better few-shot learners](#differentiable-prompt-makes-pre-trained-language-models-better-few-shot-learners) Zhang et al. (2022) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Linear (LA) | ICLR
[Finetuned Language Models are Zero-Shot Learners](#finetuned-language-models-are-zero-shot-learners) Wei et al. (2022) | Fixed | Hidden ($\mathcal{H}$) | Additive (AD) | Rule-based (RA) | ICLR
[Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134) Zhou et al. (2022) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Linear (LA) | IJCV
[Multitask Prompted Training Enables Zero-Shot Task Generalization](#multitask-prompted-training-enables-zero-shot-task-generalization) Sanh et al. (2022) | Fixed | Hidden ($\mathcal{H}$) | Additive (AD) | Rule-based (RA) | ICLR
[P-Tuning: Prompt Tuning Can Be Comparable to Fine-Tuning Across Scales and Tasks](#p-tuning-prompt-tuning-can-be-comparable-to-fine-tuning-across-scales-and-tasks) Liu et al. (2022) | Learnable | Input ($\mathcal{X}_S$) | multiplication | Linear (LA) | ACL, Volume 2: Short Papers
[PPT: Pre-trained Prompt Tuning for Few-shot Learning](#ppt-pre-trained-prompt-tuning-for-few-shot-learning) Gu et al. (2022) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Linear (LA) | Proceedings of ACL
[PTR: Prompt Tuning with Rules for Text Classification](#ptr-prompt-tuning-with-rules-for-text-classification) Han et al. (2022) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Linear (LA) | AI Open
[Spot: Better frozen model adaptation through soft prompt transfer](https://arxiv.org/abs/2110.07904) Vu et al. (2022) | Learnable | Embedding ($\mathcal{E}$) / Hidden ($\mathcal{H}$) | Concatenative (CO) | Linear (LA) | ACL
[Training language models to follow instructions with human feedback](#training-language-models-to-follow-instructions-with-human-feedback) Ouyang et al. (2022) | Fixed | Hidden ($\mathcal{H}$) | Additive (AD) | Rule-based (RA) | NeurIPS
[Deep graph reprogramming](#deep-graph-reprogramming) Jing et al. (2023) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | CVPR
[From english to more languages: Parameter-efficient model reprogramming for cross-lingual speech recognition](#from-english-to-more-languages-parameter-efficient-model-reprogramming-for-cross-lingual-speech-recognition) Yang et al. (2023) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | ICASSP
[Low-resource music genre classification with cross-modal neural model reprogramming](#low-resource-music-genre-classification-with-cross-modal-neural-model-reprogramming) Hung et al. (2023) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | ICASSP
[Reprogramming pretrained language models for antibody sequence infilling](#reprogramming-pretrained-language-models-for-antibody-sequence-infilling) Melnyk et al. (2023) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | ICML
[Universal prompt tuning for graph neural networks](#universal-prompt-tuning-for-graph-neural-networks) Fang et al. (2023) | Learnable | Input ($\mathcal{X}_S$) | Additive (AD) | Linear (LA) | NeurIPS
[Visual instruction tuning](#visual-instruction-tuning) Liu et al. (2023) | Fixed | Hidden ($\mathcal{H}$) | Additive (AD) | Rule-based (RA) | NeurIPS
[Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders](#model-reprogramming-outperforms-fine-tuning-on-out-of-distribution-data-in-text-image-encoders) Geng et al. (2024) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | SaTML
[Model reprogramming: Resource-efficient cross-domain machine learning](#model-reprogramming-resource-efficient-cross-domain-machine-learning) Chen (2024) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | AAAI
[Time-llm: Time series forecasting by reprogramming large language models](#time-llm-time-series-forecasting-by-reprogramming-large-language-models) Jin et al. (2024) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | ICLR
[Attribute-based Visual Reprogramming for Vision-Language Models](#attribute-based-visual-reprogramming-for-vision-language-models) Cai et al. (2025) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | ICLR
[Model Reprogramming Demystified: A Neural Tangent Kernel Perspective](https://arxiv.org/abs/2506.0620) Chung et al. (2025) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | arXiv preprint arXiv:2506.0620
[Refine: Inversion-free backdoor defense via model reprogramming](#refine-inversion-free-backdoor-defense-via-model-reprogramming) Chen et al. (2025) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | ICLR
[Reprogramming pretrained language models for protein sequence representation learning](#reprogramming-pretrained-language-models-for-protein-sequence-representation-learning) Vinod et al. (2025) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | Digital Discovery
[Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts](#understanding-model-reprogramming-for-clip-via-decoupling-visual-prompts) Cai et al. (2025) | Learnable | Hidden ($\mathcal{H}$) | Additive (AD) | Linear (LA) | ICML
