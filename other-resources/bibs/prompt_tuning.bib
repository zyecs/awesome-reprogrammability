% Prompt Tuning (PT)
@inproceedings{li2021prefix,
  title     = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author    = {Li, Xiang Lisa and Liang, Percy},
  year      = 2021,
  booktitle = {ACL/IJCNLP},
  pages     = {4582--4597}
}
@inproceedings{lester2021power,
  title     = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  author    = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  year      = 2021,
  booktitle = {EMNLP}
}
@article{qin2021learning,
  title   = {Learning how to ask: Querying LMs with mixtures of soft prompts},
  author  = {Qin, Guanghui and Eisner, Jason},
  year    = 2021,
  journal = {arXiv preprint arXiv:2104.06599}
}
@inproceedings{liu2022ptuning,
  title     = {P-Tuning: Prompt Tuning Can Be Comparable to Fine-Tuning Across Scales and Tasks},
  author    = {Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  year      = 2022,
  booktitle = {ACL, Volume 2: Short Papers},
  pages     = {61--68}
}
@article{liu2021p,
  title   = {P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author  = {Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  year    = 2021,
  journal = {arXiv preprint arXiv:2110.07602}
}
@article{vu2021spot,
  title   = {Spot: Better frozen model adaptation through soft prompt transfer},
  author  = {Vu, Tu and Lester, Brian and Constant, Noah and Al-Rfou, Rami and Cer, Daniel},
  year    = 2021,
  journal = {arXiv preprint arXiv:2110.07904}
}
@article{HAN2022182,
  title   = {PTR: Prompt Tuning with Rules for Text Classification},
  author  = {Xu Han and Weilin Zhao and Ning Ding and Zhiyuan Liu and Maosong Sun},
  year    = 2022,
  journal = {AI Open}
}
@inproceedings{gu2022ppt,
  title     = {PPT: Pre-trained Prompt Tuning for Few-shot Learning},
  author    = {Gu, Yuxian and Han, Xu and Liu, Zhiyuan and Huang, Minlie},
  year      = 2022,
  booktitle = {Proceedings of ACL}
}
@inproceedings{zhang2021differentiable,
  title     = {Differentiable prompt makes pre-trained language models better few-shot learners},
  author    = {Zhang, Ningyu and Li, Luoqiu and Chen, Xiang and Deng, Shumin and Bi, Zhen and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  year      = 2022,
  booktitle = {ICLR}
}
@inproceedings{fang2023universal,
  title     = {Universal prompt tuning for graph neural networks},
  author    = {Fang, Taoran and Zhang, Yunchao and Yang, Yang and Wang, Chunping and Chen, Lei},
  year      = 2023,
  booktitle = {NeurIPS}
}
